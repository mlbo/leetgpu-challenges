# Batch Normalization

Implement batch normalization forward pass for 2D input tensors. Given an input tensor of shape [N, C] where N is the batch size and C is the number of features, compute the normalized output using learnable scale (`gamma`) and shift (`beta`) parameters.

For each feature channel j, batch normalization computes:

$$
\begin{align}
\mu\_j &= \frac{1}{N} \sum\_{i=1}^{N} x\_{i,j} \\
\sigma\_j^2 &= \frac{1}{N} \sum\_{i=1}^{N} (x\_{i,j} - \mu\_j)^2 \\
\hat{x}\_{i,j} &= \frac{x\_{i,j} - \mu\_j}{\sqrt{\sigma\_j^2 + \epsilon}} \\
y\_{i,j} &= \gamma\_j \hat{x}\_{i,j} + \beta\_j
\end{align}
$$

## Implementation Requirements

* Use only native features (external libraries are not permitted)
* The `solve` function signature must remain unchanged
* The final result must be stored in the `output` tensor

## Example 1:

```
Input:  input = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]  (N=3, C=2)
        gamma = [1.0, 1.0]
        beta = [0.0, 0.0]
        eps = 1e-5
Output: output = [[-1.224, -1.224], [0.0, 0.0], [1.224, 1.224]]
```

## Example 2:

```
Input:  input = [[0.0, 1.0], [2.0, 3.0]]  (N=2, C=2)
        gamma = [2.0, 0.5]
        beta = [1.0, -1.0]
        eps = 1e-5
Output: output = [[-1.0, -1.5], [3.0, -0.5]]
```

## Constraints

* 1 <= `N` <= 10,000
* 1 <= `C` <= 1,024
* `eps` = 1e-5
* -100.0 <= input values <= 100.0
* 0.1 <= gamma values <= 10.0
* -10.0 <= beta values <= 10.0
* Performance is measured with `N` = 5,000